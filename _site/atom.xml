<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Pierre Osselin</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-09-09T22:27:44+02:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Efficient community detection in sparse networks with non-backtracking random walkers</title>
   <link href="http://localhost:4000/2019/08/25/Networks/"/>
   <updated>2019-08-25T00:00:00+02:00</updated>
   <id>http://localhost:4000/2019/08/25/Networks</id>
   <content type="html">&lt;p&gt;Currently working on it.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dirichlet Process for mixture modeling and Application</title>
   <link href="http://localhost:4000/2019/08/25/Dirichlet-Process-For-Mixture/"/>
   <updated>2019-08-25T00:00:00+02:00</updated>
   <id>http://localhost:4000/2019/08/25/Dirichlet-Process-For-Mixture</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Parametric model suffer over-under fitting and are subject to model selection. The bayesian nonparametric framework offer an alternative with a model complexity that adapts to the number of data in an automatic and scientifically founded manner.&lt;/p&gt;

&lt;p&gt;Assuming we have infinite, exchangeable data
&lt;script type=&quot;math/tex&quot;&gt;y_{1}, y_{2}, ...&lt;/script&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/De_Finetti%27s_theorem&quot;&gt;De Finetti’s theorem&lt;/a&gt; ensures the existence of a distribution
&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;
such that the predictive distribution is given by
&lt;script type=&quot;math/tex&quot;&gt;p(y_{1}, ..., y_{n}) = \int_{\Omega} p(y_{1}, ..., y_{n} | \theta)dG(\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;How can we know &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; ? The main idea is to consider &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; as a parameter to infer that lives in a space of distributions that put probability mass on sets
&lt;script type=&quot;math/tex&quot;&gt;A \subset \Omega&lt;/script&gt;
. Bayesian inference then requires to elicitate a prior on &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; and allows us to define quantities such as &lt;script type=&quot;math/tex&quot;&gt;E(G(A))&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Var(G(A))&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;G(A)|y_{1}, ..., y_{n}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;With a specified observation model
&lt;script type=&quot;math/tex&quot;&gt;p(\theta | y)&lt;/script&gt;
 the posterior becomes
 &lt;script type=&quot;math/tex&quot;&gt;p(\theta | y) \propto p(y|\theta)\int_{\mathcal{G}}p(\theta | G)\pi(dG)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Prior elicitation and careful modeling of the observation process still matter. The specified nonparametric model will have some structure that won’t adapt to every form of model mispecification.&lt;/p&gt;

&lt;p&gt;Here we will review one nonparametric model, the Dirichlet Process, and implement it in the case of clustering.&lt;/p&gt;

&lt;h2 id=&quot;the-dirichlet-process&quot;&gt;The Dirichlet process&lt;/h2&gt;

&lt;h3 id=&quot;formal-definition&quot;&gt;Formal Definition&lt;/h3&gt;

&lt;p&gt;The Dirichlet process is a stochastic process whose sample paths are probability measures. Given a distribution &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; and a positive parameter &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, we have, for any finite partition &lt;script type=&quot;math/tex&quot;&gt;A_{1}, ..., A_{n}&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G \sim DP(\alpha, H)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(G(A_{1}), ..., G(A_{n})) \sim Dirichlet(\alpha H(A_{1}), ..., \alpha H(A_{n}))&lt;/script&gt;

&lt;p&gt;We have
&lt;script type=&quot;math/tex&quot;&gt;E(G(A)) = H(A)&lt;/script&gt;
 and &lt;script type=&quot;math/tex&quot;&gt;V(G(A)) = \frac{G(A)(1 - G(A))}{\alpha + 1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Hence, &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; can be interpreted as the centered distribution and &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; the precision towards the centered distribution. This define the prior distribution for our nonparametric model.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_{i} \sim G&lt;/script&gt; are i.i.d, however, marginalized over &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; they are not independant, in particular, we can show that :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G | \theta_{1},..., \theta_{n} \sim DP \Bigg( n + \alpha, \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha} \Bigg)&lt;/script&gt;

&lt;p&gt;And, marginalized over &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{n+1} | \theta_{1},..., \theta_{n} \sim \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha}&lt;/script&gt;

&lt;p&gt;These equations gives insight into a nex representation of the parameters that we will explore now.&lt;/p&gt;

&lt;h3 id=&quot;the-chinese-restaurant-process&quot;&gt;The Chinese Restaurant process&lt;/h3&gt;

&lt;p&gt;As the previous equations show, we don’t expect all the &lt;script type=&quot;math/tex&quot;&gt;\theta_{i}&lt;/script&gt; to have distinct values, we can thus reinterpret our parameters &lt;script type=&quot;math/tex&quot;&gt;(\theta_{i})_{i = 1:n}&lt;/script&gt; to the parameters &lt;script type=&quot;math/tex&quot;&gt;\{ (\theta_{i}^{*})_{i = 1:K}, (S_{i})_{i = 1:K}\}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\theta_{i}^{*} \sim H&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(S_{1}, ..., S_{K})&lt;/script&gt; is a partition of &lt;script type=&quot;math/tex&quot;&gt;[n]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;S_{k}&lt;/script&gt; corresponds to the labels &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\theta_{i} = \theta_{k}^{*}&lt;/script&gt;. From this interpretation, we can reformulate our previous equation to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\theta_{n+1} | \theta_{1},..., \theta_{n} = \left\{
    \begin{array}{ll}
        \theta_{k}^{*} &amp; \mbox{with probability } \frac{|S_{k}|}{n + \alpha} \\
        \theta_{K + 1}^{*} &amp; \mbox{with probability } \frac{\alpha}{n + \alpha}
    \end{array}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;This operation is called the “Chinese restaurant process”, where the &lt;script type=&quot;math/tex&quot;&gt;(\theta_{i})_{i}&lt;/script&gt; can be interpreted as clients entering a restaurant, and choosing a table with a probability proportional to the number of clients sitting at this table, and a probability to choose a new table proportional to &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By following this iterative process we can directly infer the joint probability of the reparametrization:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(((\theta_{i}^{*}), S)) = P_{\alpha}(S) \times \prod\limits_{k = 1}^{K} H(\theta_{k}^{*})&lt;/script&gt;

&lt;p&gt;Where
&lt;script type=&quot;math/tex&quot;&gt;P_{\alpha}(S) = \frac{\Gamma(\alpha) \alpha^{K} \prod\limits_{k = 1}^{K}\Gamma(n_{k})}{\Gamma(\alpha + n)}&lt;/script&gt; can be directly inferred by developping the chinese restaurant process and rearranging the terms.&lt;/p&gt;

&lt;h3 id=&quot;stick-breaking-process&quot;&gt;Stick Breaking Process&lt;/h3&gt;

&lt;p&gt;There exist a way to sample a distribution &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;DP(\alpha, H)&lt;/script&gt; called the “stick breaking process” that works as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sample &lt;script type=&quot;math/tex&quot;&gt;(w_{i})_{i = 1,...} \sim Beta(1,\alpha)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Sample &lt;script type=&quot;math/tex&quot;&gt;(\theta_{i}^{*})_{i = 1,...} \sim H&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;\pi_{i}(w) = w_{i}*\prod\limits_{j = 1}^{i - 1}(1 - w_{j})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Return &lt;script type=&quot;math/tex&quot;&gt;G = \sum\limits_{i = 1}^{\infty} \pi_{i}(w)\delta_{\theta_{i}^{*}}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;/imgs/plotBreak.png&quot; alt=&quot;Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100.&quot; /&gt;
  &lt;figcaption&gt;Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The code to reproduce this process can be found &lt;a href=&quot;https://github.com/pierreosselin/dirichlet-process&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;dirichlet-process-for-mixture&quot;&gt;Dirichlet process for Mixture&lt;/h2&gt;

&lt;h3 id=&quot;finite-mixture-model&quot;&gt;Finite Mixture Model&lt;/h3&gt;

&lt;p&gt;A finite mixture model assumes that the data come from a mixture of a finite number of distributions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi \sim Dirichlet(\frac{\alpha}{K}, ...,  \frac{\alpha}{K})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c_{n} \sim Multinomial(\pi)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{k}^{*} \sim H&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_{n} | c_{n}, (\theta_{k}^{*})_{k = 1...K} \sim f(.|\theta_{c_{n}}^{*})&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; are latent variables describing the cluster the data come from. &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the distribution the parameters of the observation model come from.&lt;/p&gt;

&lt;h3 id=&quot;dirichlet-process-for-mixture-1&quot;&gt;Dirichlet Process for Mixture&lt;/h3&gt;

&lt;p&gt;It can be showed that the marginal distribution of &lt;script type=&quot;math/tex&quot;&gt;\theta_{i}&lt;/script&gt; in the finite mixture model converges in distribution to the marginal distribution of the Dirichlet Process for mixture modeling defined by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G \sim DP(\alpha, H)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{i} \sim G&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_{i}|\theta_{i} \sim f(.|\theta_{i})&lt;/script&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;In a DP model for mixture, we assume that the data are generated in the following way:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_{i} \sim f(y_{i} | \theta_{i})&lt;/script&gt;

&lt;p&gt;With the following prior for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \sim G&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G \sim DP(\alpha, H)&lt;/script&gt;

&lt;p&gt;Hence, the Bayes rule translates into, with the representation given above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi((\theta^{*}, S) | y) \propto f(y | \theta^{*}, S) P_{\alpha}(S) \prod\limits_{k = 1}^{K}h(\theta^{*}_{k})&lt;/script&gt;

&lt;p&gt;The main techniques explored in order to perform inference are MCMC methods and Variational Inference methods. However, choosing the observation distribution
&lt;script type=&quot;math/tex&quot;&gt;f(y|\theta)&lt;/script&gt;
 to be conjugate with &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; allows us to design a Gibbs Sampler that greatly improve performances.&lt;/p&gt;

&lt;h3 id=&quot;gibbs-sampling&quot;&gt;Gibbs Sampling&lt;/h3&gt;

&lt;p&gt;A Gibbs Sampler can be designed in the following manner to sample from the full posterior distribution
&lt;script type=&quot;math/tex&quot;&gt;p(\theta^{*},S | y )&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\theta_{k}^{*} | \theta_{-k}^{*},S ,(y_{i})_{i}) \propto h(\theta_{k}^{*}) \times \prod\limits_{i \in S_{k}}f(y_{i}|\theta_{k}^{*})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P(i \in S'^{k} | S^{-i}, y, \theta^{*}_{-i}) = \left\{
    \begin{array}{ll}
        \frac{|S_{k}^{-i}|}{n - 1 + \alpha} \times f(y_{i}|\theta^{*}_{k}) &amp; \mbox{for } k = 1,...,K^{-i} \\
        \frac{\alpha}{n - 1 + \alpha} \times \int f(y_{i}|\theta^{*})dH(\theta^{*}) &amp; \mbox{for } k = K^{-i} + 1
    \end{array}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;If a new cluster is created, draw&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} \sim \pi(\theta^{*} | y_{i}) \propto f(y_{i} | \theta^{*})H(\theta^{*})&lt;/script&gt;

&lt;h3 id=&quot;conjugate-priors&quot;&gt;Conjugate priors&lt;/h3&gt;

&lt;p&gt;In the case we model our observation
&lt;script type=&quot;math/tex&quot;&gt;y_{i} | \theta_{k}^{*}&lt;/script&gt;
from a multivariate gaussian:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*}_{k} = (\boldsymbol{\mu}_{k}^{*}, \boldsymbol{\Sigma}_{k}^{*})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_{i} \sim \mathcal{N}(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma}_{k}^{*})&lt;/script&gt;

&lt;p&gt;Then a conjugate prior for this distribution is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution&quot;&gt;Normal-Inverse Wishart distribution&lt;/a&gt; with the four hyperparameters
&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma_{k}}^{*}) \sim NIW(\boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu)&lt;/script&gt;

&lt;p&gt;In this case the posterior distribution can be written:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{k}^{*} | y \sim NIW(\boldsymbol{\mu}_{n}, \lambda_{n}, \boldsymbol{\Psi}_{n}, \nu_{n})&lt;/script&gt;

&lt;p&gt;Where (Caution ! Wikipedia is wrong, check &lt;a href=&quot;https://www.ics.uci.edu/~sudderth/papers/sudderthPhD.pdf&quot;&gt;Erik B. Sudderth thesis&lt;/a&gt; instead).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu_{n}} = \frac{\lambda \boldsymbol{\mu}_{0} + n\boldsymbol{\bar{y}}}{\lambda + n}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda_{n} = \lambda + n&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nu_{n} = \nu + n&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\Psi}_{n} =  \boldsymbol{\Psi} + \sum\limits_{i = 1}^{n} y_{i}y_{i}^{T} + {\lambda} \boldsymbol{\mu_{0}}\boldsymbol{\mu_{0}}^{T} - \lambda_{n}\boldsymbol{\mu_{n}}\boldsymbol{\mu_{n}}^{T}&lt;/script&gt;

&lt;p&gt;The predictive likelihood follows a multivariate student t distribution with &lt;script type=&quot;math/tex&quot;&gt;(\nu_{n} - d + 1)&lt;/script&gt; degrees of freedom that we will approximate by moment matching by a gaussian, which is a faithful representation when &lt;script type=&quot;math/tex&quot;&gt;\nu&lt;/script&gt; increases:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y | \boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu) \sim \mathcal{N}(y; \boldsymbol{\mu}_{0}, \frac{(\lambda + 1)\nu}{\lambda(\nu - p - 1)} \boldsymbol{\Psi})&lt;/script&gt;

&lt;h2 id=&quot;simulation&quot;&gt;Simulation&lt;/h2&gt;

&lt;p&gt;The source code for the simulations can be found &lt;a href=&quot;https://github.com/pierreosselin/dirichlet-process&quot;&gt;here&lt;/a&gt;. The Gibbs Sampler with the previous priors is implemented and supports data of any dimension.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;objectDP&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DirichletMixtureConjugate&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diri&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DirichletMixtureConjugate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/40.gif&quot; style=&quot;align:center; margin: 0 auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Dirichlet Process with 120 2D Gaussian points&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/1600.gif&quot; style=&quot;align:center; margin: 0 auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Dirichlet Process with 1600 2D Gaussian points&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/4000.gif&quot; style=&quot;align:center; margin: 0 auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Dirichlet Process with 4000 2D Gaussian points&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/3D.gif&quot; style=&quot;align:center; margin: 0 auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Dirichlet Process with 1600 2D Gaussian points&lt;/p&gt;

&lt;p&gt;We can see that in these cases the state of the sampler quickly attain the expected states and stay in them consistently.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;This model allows to perform non parametric bayesian inference, however, it shifts the problem to the design of the prior which establish the size a priori of a cluster.&lt;/p&gt;

&lt;p&gt;Is it Scalable?&lt;/p&gt;

&lt;p&gt;How does it sustain dimensions?&lt;/p&gt;

&lt;p&gt;Hierarchical Dirichlet Process?&lt;/p&gt;
</content>
 </entry>
 

</feed>
