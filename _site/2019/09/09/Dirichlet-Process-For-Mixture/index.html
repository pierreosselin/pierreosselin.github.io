<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Dirichlet Process for mixture modeling and Application &middot; Pierre Osselin
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <!-- Comment to remove picture -->
    <img src="http://localhost:4000/imgs/profile.jpg" style="align:center; width:100px; height:150px; margin: 0 auto; border-radius: 50%;">
    <div class="sidebar-about">
      <h2> <!-- This was originally h1, but made font smaller with h2 -->
        <a href="http://localhost:4000">
          Pierre Osselin
        </a>
      </h2>
      <a href="https://github.com/PierreOsselin">
        <i class="fa fa-github"></i>
      </a> &nbsp;
      <a href="https://linkedin.com/in/pierre-osselin-85176412b">
        <i class="fa fa-linkedin"></i>
      </a> &nbsp;
      <a href="https://twitter.com/POsselin">
        <i class="fa fa-twitter"></i>
      </a>
      <p class="lead"></p>

    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="https://github.com/pierreosselin/pierreosselin.github.io/raw/master/CVPierreResearch.pdf">Resume</a>
    </nav>
  </div>
</div>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109900176-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109900176-1');
</script>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Dirichlet Process for mixture modeling and Application</h1>
  <span class="post-date">09 Sep 2019</span>
  <h2 id="introduction">Introduction</h2>

<p>Parametric model suffer over-under fitting and are subject to model selection. The bayesian nonparametric framework offer an alternative with a model complexity that adapts to the number of data in an automatic and scientifically founded manner.</p>

<p>Assuming we have infinite, exchangeable data
<script type="math/tex">y_{1}, y_{2}, ...</script>
<a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">De Finetti’s theorem</a> ensures the existence of a distribution
<script type="math/tex">G</script>
such that the predictive distribution is given by
<script type="math/tex">p(y_{1}, ..., y_{n}) = \int_{\Omega} p(y_{1}, ..., y_{n} | \theta)dG(\theta)</script></p>

<p>How can we know <script type="math/tex">G</script> ? The main idea is to consider <script type="math/tex">G</script> as a parameter to infer that lives in a space of distributions that put probability mass on sets
<script type="math/tex">A \subset \Omega</script>
. Bayesian inference then requires to elicitate a prior on <script type="math/tex">G</script> and allows us to define quantities such as <script type="math/tex">E(G(A))</script>, <script type="math/tex">Var(G(A))</script> or <script type="math/tex">G(A)|y_{1}, ..., y_{n}</script>.</p>

<p>With a specified observation model
<script type="math/tex">p(\theta | y)</script>
 the posterior becomes
 <script type="math/tex">p(\theta | y) \propto p(y|\theta)\int_{\mathcal{G}}p(\theta | G)\pi(dG)</script></p>

<p>Prior elicitation and careful modeling of the observation process still matter. The specified nonparametric model will have some structure that won’t adapt to every form of model mispecification.</p>

<p>Here we will review one nonparametric model, the Dirichlet Process, and implement it in the case of clustering.</p>

<h2 id="the-dirichlet-process">The Dirichlet process</h2>

<h3 id="formal-definition">Formal Definition</h3>

<p>The Dirichlet process is a stochastic process whose sample paths are probability measures. Given a distribution <script type="math/tex">H</script> and a positive parameter <script type="math/tex">\alpha</script>, we have, for any finite partition <script type="math/tex">A_{1}, ..., A_{n}</script> of <script type="math/tex">\Omega</script>:</p>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<script type="math/tex; mode=display">(G(A_{1}), ..., G(A_{n})) \sim Dirichlet(\alpha H(A_{1}), ..., \alpha H(A_{n}))</script>

<p>We have
<script type="math/tex">E(G(A)) = H(A)</script>
 and <script type="math/tex">V(G(A)) = \frac{G(A)(1 - G(A))}{\alpha + 1}</script>.</p>

<p>Hence, <script type="math/tex">H</script> can be interpreted as the centered distribution and <script type="math/tex">\alpha</script> the precision towards the centered distribution. This define the prior distribution for our nonparametric model.</p>

<p><script type="math/tex">\theta_{i} \sim G</script> are i.i.d, however, marginalized over <script type="math/tex">G</script> they are not independant, in particular, we can show that :</p>

<script type="math/tex; mode=display">G | \theta_{1},..., \theta_{n} \sim DP \Bigg( n + \alpha, \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha} \Bigg)</script>

<p>And, marginalized over <script type="math/tex">G</script>:</p>

<script type="math/tex; mode=display">\theta_{n+1} | \theta_{1},..., \theta_{n} \sim \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha}</script>

<p>These equations gives insight into a nex representation of the parameters that we will explore now.</p>

<h3 id="the-chinese-restaurant-process">The Chinese Restaurant process</h3>

<p>As the previous equations show, we don’t expect all the <script type="math/tex">\theta_{i}</script> to have distinct values, we can thus reinterpret our parameters <script type="math/tex">(\theta_{i})_{i = 1:n}</script> to the parameters <script type="math/tex">\{ (\theta_{i}^{*})_{i = 1:K}, (S_{i})_{i = 1:K}\}</script> where <script type="math/tex">\theta_{i}^{*} \sim H</script> and <script type="math/tex">(S_{1}, ..., S_{K})</script> is a partition of <script type="math/tex">[n]</script>, where <script type="math/tex">S_{k}</script> corresponds to the labels <script type="math/tex">i</script> such that <script type="math/tex">\theta_{i} = \theta_{k}^{*}</script>. From this interpretation, we can reformulate our previous equation to:</p>

<script type="math/tex; mode=display">% <![CDATA[
\theta_{n+1} | \theta_{1},..., \theta_{n} = \left\{
    \begin{array}{ll}
        \theta_{k}^{*} & \mbox{with probability } \frac{|S_{k}|}{n + \alpha} \\
        \theta_{K + 1}^{*} & \mbox{with probability } \frac{\alpha}{n + \alpha}
    \end{array}
\right. %]]></script>

<p>This operation is called the “Chinese restaurant process”, where the <script type="math/tex">(\theta_{i})_{i}</script> can be interpreted as clients entering a restaurant, and choosing a table with a probability proportional to the number of clients sitting at this table, and a probability to choose a new table proportional to <script type="math/tex">\alpha</script>.</p>

<p>By following this iterative process we can directly infer the joint probability of the reparametrization:</p>

<script type="math/tex; mode=display">\pi(((\theta_{i}^{*}), S)) = P_{\alpha}(S) \times \prod\limits_{k = 1}^{K} H(\theta_{k}^{*})</script>

<p>Where
<script type="math/tex">P_{\alpha}(S) = \frac{\Gamma(\alpha) \alpha^{K} \prod\limits_{k = 1}^{K}\Gamma(n_{k})}{\Gamma(\alpha + n)}</script> can be directly inferred by developping the chinese restaurant process and rearranging the terms.</p>

<h3 id="stick-breaking-process">Stick Breaking Process</h3>

<p>There exist a way to sample a distribution <script type="math/tex">G</script> from <script type="math/tex">DP(\alpha, H)</script> called the “stick breaking process” that works as follows:</p>

<ul>
  <li>Sample <script type="math/tex">(w_{i})_{i = 1,...} \sim Beta(1,\alpha)</script></li>
  <li>Sample <script type="math/tex">(\theta_{i}^{*})_{i = 1,...} \sim H</script></li>
  <li>Compute <script type="math/tex">\pi_{i}(w) = w_{i}*\prod\limits_{j = 1}^{i - 1}(1 - w_{j})</script></li>
  <li>Return <script type="math/tex">G = \sum\limits_{i = 1}^{\infty} \pi_{i}(w)\delta_{\theta_{i}^{*}}</script></li>
</ul>

<p>This interpretation is very insteresting, because we see that sampling a distribution according to the Dirichlet Process compute an infinite probabitily vector corresponding to parameters sampled from the base distribution <script type="math/tex">H</script>.</p>

<figure class="image">
  <img src="/imgs/plotBreak.png" alt="Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100." />
  <figcaption>Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100.</figcaption>
</figure>

<p>The code to reproduce this process can be found <a href="https://github.com/pierreosselin/dirichlet-process">here</a>.</p>
<h2 id="dirichlet-process-for-mixture">Dirichlet process for Mixture</h2>

<h3 id="finite-mixture-model">Finite Mixture Model</h3>

<p>A finite mixture model assumes that the data come from a mixture of a finite number of distributions.</p>

<script type="math/tex; mode=display">\pi \sim Dirichlet(\frac{\alpha}{K}, ...,  \frac{\alpha}{K})</script>

<script type="math/tex; mode=display">c_{n} \sim Multinomial(\pi)</script>

<script type="math/tex; mode=display">\theta_{k}^{*} \sim H</script>

<script type="math/tex; mode=display">y_{n} | c_{n}, (\theta_{k}^{*})_{k = 1...K} \sim f(.|\theta_{c_{n}}^{*})</script>

<p>Here <script type="math/tex">\pi</script> are latent variables describing the cluster the data come from. <script type="math/tex">H</script> is the distribution the parameters of the observation model come from.</p>

<h3 id="dirichlet-process-for-mixture-1">Dirichlet Process for Mixture</h3>

<p>It can be showed that the marginal distribution of <script type="math/tex">\theta_{i}</script> in the finite mixture model converges in distribution to the marginal distribution of the Dirichlet Process for mixture modeling defined by:</p>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<script type="math/tex; mode=display">\theta_{i} \sim G</script>

<script type="math/tex; mode=display">y_{i}|\theta_{i} \sim f(.|\theta_{i})</script>

<h2 id="inference">Inference</h2>

<p>In a DP model for mixture, we assume that the data are generated in the following way:</p>

<script type="math/tex; mode=display">y_{i} \sim f(y_{i} | \theta_{i})</script>

<p>With the following prior for <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\theta \sim G</script>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<p>Hence, the Bayes rule translates into, with the representation given above:</p>

<script type="math/tex; mode=display">\pi((\theta^{*}, S) | y) \propto f(y | \theta^{*}, S) P_{\alpha}(S) \prod\limits_{k = 1}^{K}h(\theta^{*}_{k})</script>

<p>The main techniques explored in order to perform inference are MCMC methods and Variational Inference methods. However, choosing the observation distribution
<script type="math/tex">f(y|\theta)</script>
 to be conjugate with <script type="math/tex">H</script> allows us to design a Gibbs Sampler that greatly improve performances.</p>

<h3 id="gibbs-sampling">Gibbs Sampling</h3>

<p>A Gibbs Sampler can be designed in the following manner to sample from the full posterior distribution
<script type="math/tex">p(\theta^{*},S | y )</script>:</p>

<script type="math/tex; mode=display">\pi(\theta_{k}^{*} | \theta_{-k}^{*},S ,(y_{i})_{i}) \propto h(\theta_{k}^{*}) \times \prod\limits_{i \in S_{k}}f(y_{i}|\theta_{k}^{*})</script>

<script type="math/tex; mode=display">% <![CDATA[
P(i \in S'^{k} | S^{-i}, y, \theta^{*}_{-i}) = \left\{
    \begin{array}{ll}
        \frac{|S_{k}^{-i}|}{n - 1 + \alpha} \times f(y_{i}|\theta^{*}_{k}) & \mbox{for } k = 1,...,K^{-i} \\
        \frac{\alpha}{n - 1 + \alpha} \times \int f(y_{i}|\theta^{*})dH(\theta^{*}) & \mbox{for } k = K^{-i} + 1
    \end{array}
\right. %]]></script>

<p>If a new cluster is created, draw</p>

<script type="math/tex; mode=display">\theta^{*} \sim \pi(\theta^{*} | y_{i}) \propto f(y_{i} | \theta^{*})H(\theta^{*})</script>

<h3 id="conjugate-priors">Conjugate priors</h3>

<p>In the case we model our observation
<script type="math/tex">y_{i} | \theta_{k}^{*}</script>
from a multivariate gaussian:</p>

<script type="math/tex; mode=display">\theta^{*}_{k} = (\boldsymbol{\mu}_{k}^{*}, \boldsymbol{\Sigma}_{k}^{*})</script>

<script type="math/tex; mode=display">y_{i} \sim \mathcal{N}(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma}_{k}^{*})</script>

<p>Then a conjugate prior for this distribution is the <a href="https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution">Normal-Inverse Wishart distribution</a> with the four hyperparameters
<script type="math/tex">(\boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu)</script>:</p>

<script type="math/tex; mode=display">(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma_{k}}^{*}) \sim NIW(\boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu)</script>

<p>In this case the posterior distribution can be written:</p>

<script type="math/tex; mode=display">\theta_{k}^{*} | y \sim NIW(\boldsymbol{\mu}_{n}, \lambda_{n}, \boldsymbol{\Psi}_{n}, \nu_{n})</script>

<p>Where :</p>

<script type="math/tex; mode=display">\boldsymbol{\mu_{n}} = \frac{\lambda \boldsymbol{\mu}_{0} + n\boldsymbol{\bar{y}}}{\lambda + n}</script>

<script type="math/tex; mode=display">\lambda_{n} = \lambda + n</script>

<script type="math/tex; mode=display">\nu_{n} = \nu + n</script>

<script type="math/tex; mode=display">\boldsymbol{\Psi}_{n} =  \boldsymbol{\Psi} + \sum\limits_{i = 1}^{n} y_{i}y_{i}^{T} + {\lambda} \boldsymbol{\mu_{0}}\boldsymbol{\mu_{0}}^{T} - \lambda_{n}\boldsymbol{\mu_{n}}\boldsymbol{\mu_{n}}^{T}</script>

<p>The predictive likelihood follows a multivariate student t distribution with <script type="math/tex">(\nu_{n} - d + 1)</script> degrees of freedom that we will approximate by moment matching by a gaussian, which is a faithful representation (according to the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a>) when <script type="math/tex">\nu</script> increases:</p>

<script type="math/tex; mode=display">p(y | \boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu) \sim \mathcal{N}(y; \boldsymbol{\mu}_{0}, \frac{(\lambda + 1)\nu}{\lambda(\nu - p - 1)} \boldsymbol{\Psi})</script>

<h2 id="simulation">Simulation</h2>

<p>The source code for the simulations can be found <a href="https://github.com/pierreosselin/dirichlet-process">here</a>. The Gibbs Sampler with the previous priors is implemented and supports data of any dimension.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">objectDP</span> <span class="kn">import</span> <span class="n">DirichletMixtureConjugate</span>
<span class="n">diri</span> <span class="o">=</span> <span class="n">DirichletMixtureConjugate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">diri</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">vis</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/imgs/40.gif" style="align:center; margin: 0 auto; width:500px;" /></p>
<p style="text-align: center; font-style: italic; font-size: 80%;">Dirichlet Process with 120 2D Gaussian points</p>

<p><img src="http://localhost:4000/imgs/1600.gif" style="align:center; margin: 0 auto; width:500px;" /></p>
<p style="text-align: center; font-style: italic; font-size: 80%;">Dirichlet Process with 1600 2D Gaussian points</p>

<p><img src="http://localhost:4000/imgs/4000.gif" style="align:center; margin: 0 auto; width:500px;" /></p>
<p style="text-align: center; font-style: italic; font-size: 80%;">Dirichlet Process with 4000 2D Gaussian points</p>

<p><img src="http://localhost:4000/imgs/3D.gif" style="align:center; margin: 0 auto; width:500px;" /></p>
<p style="text-align: center; font-style: italic; font-size: 80%;">Dirichlet Process with 1600 2D Gaussian points</p>

<p>We can see that in these cases the state of the sampler quickly attain the expected states and stay in them consistently. Of course, in a Bayesian setting, we should, instead of displaying the state of the Markov Chain, study the posterior distribution of certain quantity of interests such as the number of clusters, and then infer chosen estimators, such as the mode of the posterior mean. Here we suppose that the states quickly attain the mode of the posterior.</p>

<h2 id="discussion">Discussion</h2>

<p>This model allows to perform non parametric bayesian inference, however, it shifts the problem to the design of the prior which establish the size a priori of a cluster. It is interesting to note that one particular modification of the Dirichlet Process, calles the Pitman-Yor process has attracted attention, especially because it gives a power-law like distribution over the parameters (the infinite probability vector we talked about), and thus gives more flexibility for modelling certain type of data with the same behaviour.</p>

<h2 id="note-on-model-extension">Note on model extension.</h2>

<h3 id="taking-one-additional-step-of-abstraction-the-hierarchical-dirichlet-process">Taking one additional step of abstraction: The Hierarchical Dirichlet Process</h3>

<p>Imagine the data comes from different groups. For instance, in the problem of Topic Modelling, which consists in discovering the abstract “topics” that occur in a collection of documents, one can model a document as a mixture of latent topics, and each topic as a mixture of words. In this model (described by the <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation</a>), the model has a Hierarchical structure, ie, the distribution of words in a document is conditioned by the distribution of the document over topics the word belongs to. Certain Hierarchical structure, whose hierarchy can be arbitrarily extended and represented in a tree structure, can be naturally modeled by Hierarchical statistical model, such as the Hierarchical Dirichlet Process among others, described as follows:</p>

<script type="math/tex; mode=display">G_{0} \sim DP(\gamma, H)</script>

<script type="math/tex; mode=display">\forall j \in \{1,...,J\} G_{j} \sim DP(\alpha_{0}, G_{0})</script>

<p>Here, <script type="math/tex">J</script> represents the number of groups (the documents in the case of topic modelling). Whithin this group, the configuration is the same as a classic Dirichlet Process:</p>

<script type="math/tex; mode=display">\forall i \in \{1,...,N_{j}\} \theta_{i,j} \sim G_{j}</script>

<script type="math/tex; mode=display">x_{i,j} \sim F(\theta_{i,j})</script>

<p>Where <script type="math/tex">F</script> is the observation model. In this model, an extension of Chinese Restaurant Process and the Stick Breaking Process exist to treat this configuration. One can see the <a href="https://www.stat.berkeley.edu/~aldous/206-Exch/Papers/hierarchical_dirichlet.pdf">original paper</a> for more details.</p>

</div>

<!-- mathjax (to render equations) -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2019/09/08/Rank/">
            Modeling and analysis of social network data from rank preferences
            <small>08 Sep 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/09/07/Networks/">
            Efficient community detection in sparse networks with non-backtracking random walkers
            <small>07 Sep 2019</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = 'http://localhost:4000/2019/09/09/Dirichlet-Process-For-Mixture/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = '/2019/09/09/Dirichlet-Process-For-Mixture/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pierreosselin.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>

  </body>
</html>
