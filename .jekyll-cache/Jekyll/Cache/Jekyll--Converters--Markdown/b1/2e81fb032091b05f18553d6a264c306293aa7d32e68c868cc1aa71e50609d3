I"./<h2 id="introduction">Introduction</h2>

<p>Parametric model suffer over-under fitting and are subject to model selection. The bayesian nonparametric framework offer an alternative with a model complexity that adapts to the number of data in an automatic and scientifically founded manner.</p>

<p>Assuming we have infinite, exchangeable data
<script type="math/tex">y_{1}, y_{2}, ...</script>
<a href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem">De Finetti’s theorem</a> ensures the existence of a distribution
<script type="math/tex">G</script>
such that the predictive distribution is given by
<script type="math/tex">p(y_{1}, ..., y_{n}) = \int_{\Omega} p(y_{1}, ..., y_{n} | \theta)dG(\theta)</script></p>

<p>How can we know <script type="math/tex">G</script> ? The main idea is to consider <script type="math/tex">G</script> as a parameter to infer that lives in a space of distributions that put probability mass on sets
<script type="math/tex">A \subset \Omega</script>
. Bayesian inference then requires to elicitate a prior on <script type="math/tex">G</script> and allows us to define quantities such as <script type="math/tex">E(G(A))</script>, <script type="math/tex">Var(G(A))</script> or <script type="math/tex">G(A)|y_{1}, ..., y_{n}</script>.</p>

<p>With a specified observation model
<script type="math/tex">p(\theta | y)</script>
 the posterior becomes
 <script type="math/tex">p(\theta | y) \propto p(y|\theta)\int_{\mathcal{G}}p(\theta | G)\pi(dG)</script></p>

<p>Prior elicitation and careful modeling of the observation process still matter. The specified nonparametric model will have some structure that won’t adapt to every form of model mispecification.</p>

<p>Here we will review one nonparametric model, the Dirichlet Process, and implement it in the case of clustering.</p>

<h2 id="the-dirichlet-process">The Dirichlet process</h2>

<h3 id="formal-definition">Formal Definition</h3>

<p>The Dirichlet process is a stochastic process whose sample paths are probability measures. Given a distribution <script type="math/tex">H</script> and a positive parameter <script type="math/tex">\alpha</script>, we have, for any finite partition <script type="math/tex">A_{1}, ..., A_{n}</script> of <script type="math/tex">\Omega</script>:</p>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<script type="math/tex; mode=display">(G(A_{1}), ..., G(A_{n})) \sim Dirichlet(\alpha H(A_{1}), ..., \alpha H(A_{n}))</script>

<p>We have
<script type="math/tex">E(G(A)) = H(A)</script>
 and <script type="math/tex">V(G(A)) = \frac{G(A)(1 - G(A))}{\alpha + 1}</script>.</p>

<p>Hence, <script type="math/tex">H</script> can be interpreted as the centered distribution and <script type="math/tex">\alpha</script> the precision towards the centered distribution. This define the prior distribution for our nonparametric model.</p>

<p><script type="math/tex">\theta_{i} \sim G</script> are i.i.d, however, marginalized over <script type="math/tex">G</script> they are not independant, in particular, we can show that :</p>

<script type="math/tex; mode=display">G | \theta_{1},..., \theta_{n} \sim DP \Bigg( n + \alpha, \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha} \Bigg)</script>

<p>And, marginalized over <script type="math/tex">G</script>:</p>

<script type="math/tex; mode=display">\theta_{n+1} | \theta_{1},..., \theta_{n} \sim \frac{\alpha H + \sum\limits_{i = 1}^{n} \delta_{\theta_{i}}}{n + \alpha}</script>

<p>These equations gives insight into a nex representation of the parameters that we will explore now.</p>

<h3 id="the-chinese-restaurant-process">The Chinese Restaurant process</h3>

<p>As the previous equations show, we don’t expect all the <script type="math/tex">\theta_{i}</script> to have distinct values, we can thus reinterpret our parameters <script type="math/tex">(\theta_{i})_{i = 1:n}</script> to the parameters <script type="math/tex">\{ (\theta_{i}^{*})_{i = 1:K}, (S_{i})_{i = 1:K}\}</script> where <script type="math/tex">\theta_{i}^{*} \sim H</script> and <script type="math/tex">(S_{1}, ..., S_{K})</script> is a partition of <script type="math/tex">[n]</script>, where <script type="math/tex">S_{k}</script> corresponds to the labels <script type="math/tex">i</script> such that <script type="math/tex">\theta_{i} = \theta_{k}^{*}</script>. From this interpretation, we can reformulate our previous equation to:</p>

<script type="math/tex; mode=display">% <![CDATA[
\theta_{n+1} | \theta_{1},..., \theta_{n} = \left\{
    \begin{array}{ll}
        \theta_{k}^{*} & \mbox{with probability } \frac{|S_{k}|}{n + \alpha} \\
        \theta_{K + 1}^{*} & \mbox{with probability } \frac{\alpha}{n + \alpha}
    \end{array}
\right. %]]></script>

<p>This operation is called the “Chinese restaurant process”, where the <script type="math/tex">(\theta_{i})_{i}</script> can be interpreted as clients entering a restaurant, and choosing a table with a probability proportional to the number of clients sitting at this table, and a probability to choose a new table proportional to <script type="math/tex">\alpha</script>.</p>

<p>By following this iterative process we can directly infer the joint probability of the reparametrization:</p>

<script type="math/tex; mode=display">\pi(((\theta_{i}^{*}), S)) = P_{\alpha}(S) \times \prod\limits_{k = 1}^{K} H(\theta_{k}^{*})</script>

<p>Where
<script type="math/tex">P_{\alpha}(S) = \frac{\Gamma(\alpha) \alpha^{K} \prod\limits_{k = 1}^{K}\Gamma(n_{k})}{\Gamma(\alpha + n)}</script> can be directly inferred by developping the chinese restaurant process and rearranging the terms.</p>

<h3 id="stick-breaking-process">Stick Breaking Process</h3>

<p>There exist a way to sample a distribution <script type="math/tex">G</script> from <script type="math/tex">DP(\alpha, H)</script> called the “stick breaking process” that works as follows:</p>

<ul>
  <li>Sample <script type="math/tex">(w_{i})_{i = 1,...} \sim Beta(1,\alpha)</script></li>
  <li>Sample <script type="math/tex">(\theta_{i}^{*})_{i = 1,...} \sim H</script></li>
  <li>Compute <script type="math/tex">\pi_{i}(w) = w_{i}*\prod\limits_{j = 1}^{i - 1}(1 - w_{j})</script></li>
  <li>Return <script type="math/tex">G = \sum\limits_{i = 1}^{\infty} \pi_{i}(w)\delta_{\theta_{i}^{*}}</script></li>
</ul>

<figure class="image">
  <img src="/imgs/plotBreak.png" alt="Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100." />
  <figcaption>Exemple of samples from the Dirichlet Process centered on a Gaussian with alpha equal to 1 ,10 and 100.</figcaption>
</figure>

<p>The code to reproduce this process can be found <a href="https://github.com/pierreosselin/dirichlet-process">here</a>.</p>
<h2 id="dirichlet-process-for-mixture">Dirichlet process for Mixture</h2>

<h3 id="finite-mixture-model">Finite Mixture Model</h3>

<p>A finite mixture model assumes that the data come from a mixture of a finite number of distributions.</p>

<script type="math/tex; mode=display">\pi \sim Dirichlet(\frac{\alpha}{K}, ...,  \frac{\alpha}{K})</script>

<script type="math/tex; mode=display">c_{n} \sim Multinomial(\pi)</script>

<script type="math/tex; mode=display">\theta_{k}^{*} \sim H</script>

<script type="math/tex; mode=display">y_{n} | c_{n}, (\theta_{k}^{*})_{k = 1...K} \sim f(.|\theta_{c_{n}}^{*})</script>

<p>Here <script type="math/tex">\pi</script> are latent variables describing the cluster the data come from. <script type="math/tex">H</script> is the distribution the parameters of the observation model come from.</p>

<h3 id="dirichlet-process-for-mixture-1">Dirichlet Process for Mixture</h3>

<p>It can be showed that the marginal distribution of <script type="math/tex">\theta_{i}</script> in the finite mixture model converges in distribution to the marginal distribution of the Dirichlet Process for mixture modeling defined by:</p>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<script type="math/tex; mode=display">\theta_{i} \sim G</script>

<script type="math/tex; mode=display">y_{i}|\theta_{i} \sim f(.|\theta_{i})</script>

<h2 id="inference">Inference</h2>

<p>In a DP model for mixture, we assume that the data are generated in the following way:</p>

<script type="math/tex; mode=display">y_{i} \sim f(y_{i} | \theta_{i})</script>

<p>With the following prior for <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\theta \sim G</script>

<script type="math/tex; mode=display">G \sim DP(\alpha, H)</script>

<p>Hence, the Bayes rule translates into, with the representation given above:</p>

<script type="math/tex; mode=display">\pi((\theta^{*}, S) | y) \propto f(y | \theta^{*}, S) P_{\alpha}(S) \prod\limits_{k = 1}^{K}h(\theta^{*}_{k})</script>

<p>The main techniques explored in order to perform inference are MCMC methods and Variational Inference methods. However, choosing the observation distribution
<script type="math/tex">f(y|\theta)</script>
 to be conjugate with <script type="math/tex">H</script> allows us to design a Gibbs Sampler that greatly improve performances.</p>

<h3 id="gibbs-sampling">Gibbs Sampling</h3>

<p>Choosing f and H such that conjugate priors allow us to derive a Gibbs sampler.</p>

<script type="math/tex; mode=display">\pi(\theta_{k}^{*} | \theta_{-k}^{*}, (y_{i})_{i}) \propto h(\theta_{k}^{*}) \times \prod\limits_{i \in S_{k}}f(y_{i}|\theta_{k}^{*})</script>

<script type="math/tex; mode=display">% <![CDATA[
P(i \in S'^{k} | S^{-i}, y, \theta^{*}_{-i}) = \left\{
    \begin{array}{ll}
        \frac{|S_{k}^{-i}|}{n - 1 + \alpha} \times f(y_{i}|\theta^{*}_{k}) & \mbox{for } k = 1,...,K^{-i} \\
        \frac{\alpha}{n - 1 + \alpha} \times \int f(y_{i}|\theta^{*})dH(\theta^{*}) & \mbox{for } k = K^{-i} + 1
    \end{array}
\right. %]]></script>

<p>If a new cluster is created, draw
<script type="math/tex">\theta^{*} \sim \pi(\theta^{*} | y_{i}) \propto f(y_{i} | \theta^{*})H(\theta^{*})</script></p>

<h3 id="conjugate-priors">Conjugate priors</h3>

<p>with</p>

<script type="math/tex; mode=display">\theta^{*}_{k} = (\boldsymbol{\mu}_{k}^{*}, \boldsymbol{\Sigma}_{k}^{*})</script>

<script type="math/tex; mode=display">y_{i} \sim \mathcal{N}(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma}_{k}^{*})</script>

<script type="math/tex; mode=display">(\boldsymbol{\mu_{k}}^{*}, \boldsymbol{\Sigma_{k}}^{*}) \sim NIW(\boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu)</script>

<p>In this case we have:
<script type="math/tex">\theta_{k}^{*} | y \sim NIW(\boldsymbol{\mu}_{n}, \lambda_{n}, \boldsymbol{\Psi}_{n}, \nu_{n})</script></p>

<p>Where</p>

<script type="math/tex; mode=display">\boldsymbol{\mu_{n}} = \frac{\lambda \boldsymbol{\mu}_{0} + n\boldsymbol{\bar{y}}}{\lambda + n}</script>

<script type="math/tex; mode=display">\lambda_{n} = \lambda + n</script>

<script type="math/tex; mode=display">\nu_{n} = \nu + n</script>

<script type="math/tex; mode=display">\boldsymbol{\Psi}_{n} =  \frac{\nu}{\nu + n} \boldsymbol{\Psi} + \boldsymbol{S} + \frac{\lambda n}{\lambda + n} (\boldsymbol{\bar{y}} - \boldsymbol{\mu_{0}})(\boldsymbol{\bar{y}} - \boldsymbol{\mu}_{0})^{T}</script>

<p>With</p>

<script type="math/tex; mode=display">\boldsymbol{S} = \sum\limits_{i = 1}^{n} (\boldsymbol{y}_{i} - \boldsymbol{\bar{y}})(\boldsymbol{y}_{i} - \boldsymbol{\bar{y}})^{T}</script>

<p>The predictive likelihood follows a multivariate student t distribution with <script type="math/tex">(\nu_{n} - d + 1)</script> that we will approximate by moment matching by a gaussian:</p>

<script type="math/tex; mode=display">p(y | \boldsymbol{\mu}_{0}, \lambda, \boldsymbol{\Psi}, \nu) \sim \mathcal{N}(y; \boldsymbol{\mu}_{0}, \frac{(\lambda + 1)\nu}{\lambda(\nu - p - 1)} \boldsymbol{\Psi})</script>

<h2 id="discussion">Discussion</h2>

<p>This model allows to perforw non parametric bayesian inference, however, it shifts the problem to the design of the prior which establish the size a priori of a cluster.</p>

<h2 id="ressources">Ressources</h2>

<p>This blog as been</p>
:ET